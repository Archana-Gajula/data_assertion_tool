{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60c84fee",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Data assertion Using pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d75c3e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col, array, when, array_remove, lit, size, array_contains\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b854bdad",
   "metadata": {},
   "source": [
    "specify the files needs to be compared and primary key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1eaaa006",
   "metadata": {},
   "outputs": [],
   "source": [
    "    expected_file_path = '/Users/archanagajula/IdeaProjects/pyspark_data_assertion/data/expected_data.csv'\n",
    "    actual_file_path = '/Users/archanagajula/IdeaProjects/pyspark_data_assertion/data/actual_data.csv'\n",
    "    primary_key_column = 'vendor_id'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d3d2c1",
   "metadata": {},
   "source": [
    "start the spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6cbecc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "    spark = SparkSession.builder.getOrCreate()\n",
    "    spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55730dfc",
   "metadata": {},
   "source": [
    "read files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "388be0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "    actual_df = spark.read.csv(actual_file_path, sep=',', header=True)\n",
    "    expected_df = spark.read.csv(expected_file_path, sep=',', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8bd17376",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of all records from expected: 7\n",
      "Distinct Count of records from expected: 7\n",
      "Count of all records from actual: 7\n",
      "Distinct Count of records from actual: 7\n"
     ]
    }
   ],
   "source": [
    "    print(\"Count of all records from expected:\", expected_df.count())\n",
    "    print(\"Distinct Count of records from expected:\", expected_df.distinct().count())\n",
    "    print(\"Count of all records from actual:\", actual_df.count())\n",
    "    print(\"Distinct Count of records from actual:\", actual_df.distinct().count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f5e6b020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of records that are in both: 4\n"
     ]
    }
   ],
   "source": [
    "    common_records_df = expected_df.intersect(actual_df)\n",
    "    print(\"Count of records that are in both:\", common_records_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6fd322e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "    only_expected_df = expected_df.subtract(actual_df)\n",
    "\n",
    "    only_actual_df = actual_df.subtract(expected_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0496dc9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of records that are only in expected: 3\n"
     ]
    }
   ],
   "source": [
    "    print('Count of records that are only in expected:',only_expected_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7b2f4f42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of records that are only in actual: 3\n"
     ]
    }
   ],
   "source": [
    "    print('Count of records that are only in actual:',only_actual_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bc318307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some non common records from expected:\n",
      "+---------+-------------------+-------------------+---------------+-------------+------------+------------------+------------------+-------------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+\n",
      "|vendor_id|pickup_datetime    |dropoff_datetime   |passenger_count|trip_distance|rate_code_id|store_and_fwd_flag|pickup_location_id|dropoff_location_id|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|\n",
      "+---------+-------------------+-------------------+---------------+-------------+------------+------------------+------------------+-------------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+\n",
      "|7        |2019-01-27 18:49:51|2019-01-27 19:49:51|1              |1.3          |1           |N                 |144               |45                 |2           |10.5       |0.0  |null   |null      |null        |null                 |null        |null                |\n",
      "+---------+-------------------+-------------------+---------------+-------------+------------+------------------+------------------+-------------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "    print(\"Some non common records from expected:\")\n",
    "    expected_non_matching_records = only_expected_df.join(only_actual_df, on=primary_key_column, how='left_anti').select(\n",
    "        only_expected_df[\"*\"])\n",
    "    expected_non_matching_records.orderBy(F.col(primary_key_column).asc()).show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6411499e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some non common records from actual:\n",
      "+---------+-------------------+-------------------+---------------+-------------+------------+------------------+------------------+-------------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+\n",
      "|vendor_id|pickup_datetime    |dropoff_datetime   |passenger_count|trip_distance|rate_code_id|store_and_fwd_flag|pickup_location_id|dropoff_location_id|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|\n",
      "+---------+-------------------+-------------------+---------------+-------------+------------+------------------+------------------+-------------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+\n",
      "|8        |2019-01-26 18:20:17|2019-01-26 18:39:27|1              |1.3          |1           |N                 |144               |45                 |2           |10.5       |0.0  |0.5    |0.0       |0.0         |0.3                  |11.3        |0.0                 |\n",
      "+---------+-------------------+-------------------+---------------+-------------+------------+------------------+------------------+-------------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "    print(\"Some non common records from actual:\")\n",
    "    actual_non_matching_records = only_actual_df.join(only_expected_df, on=primary_key_column, how='left_anti').select(\n",
    "        only_actual_df[\"*\"])\n",
    "    actual_non_matching_records.orderBy(F.col(primary_key_column).asc()).show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45bdc9dc",
   "metadata": {},
   "source": [
    "compare records column wise and adding non matching column names into array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7e825561",
   "metadata": {},
   "outputs": [],
   "source": [
    "    mismatch_columns = [when(expected_df[c] != actual_df[c], lit(c)).otherwise(\"\") for c in expected_df.columns]\n",
    "    select_exp = [col(primary_key_column), array_remove(array(*mismatch_columns), \"\").alias(\"mismatch_columns\")]\n",
    "    mismatch_columns_df = expected_df.join(actual_df,\n",
    "                                           on=primary_key_column, how='inner').select(select_exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "469b6bb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of records having mismatch: 2\n",
      "Count of records matching: 4\n"
     ]
    }
   ],
   "source": [
    "    print('Count of records having mismatch:', mismatch_columns_df.filter(size(\"mismatch_columns\") > 0).count())\n",
    "    print('Count of records matching:', mismatch_columns_df.filter(size(\"mismatch_columns\") == 0).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a0abc226",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------------------+\n",
      "|vendor_id|mismatch_columns                |\n",
      "+---------+--------------------------------+\n",
      "|3        |[passenger_count, trip_distance]|\n",
      "|6        |[pickup_datetime]               |\n",
      "+---------+--------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "    mismatch_columns_df.filter(size(\"mismatch_columns\") > 0).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "25b214c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_df.createOrReplaceTempView('actual_table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "033eddd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------+-------------------+---------------+-------------+------------+------------------+------------------+-------------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+\n",
      "|vendor_id|    pickup_datetime|   dropoff_datetime|passenger_count|trip_distance|rate_code_id|store_and_fwd_flag|pickup_location_id|dropoff_location_id|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|\n",
      "+---------+-------------------+-------------------+---------------+-------------+------------+------------------+------------------+-------------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+\n",
      "|        1|2019-01-15 03:36:12|2019-01-15 03:42:19|              1|          1.0|           1|                 N|               230|                 48|           1|        6.5|  0.5|    0.5|      1.95|         0.0|                  0.3|        9.75|                null|\n",
      "|        2|2019-01-25 18:20:32|2019-01-25 18:26:55|              1|          0.8|           1|                 N|               112|                112|           1|        6.0|  1.0|    0.5|      1.55|         0.0|                  0.3|        9.35|                 0.0|\n",
      "|        3|2019-01-05 06:47:31|2019-01-05 06:52:19|              2|          2.0|           1|                 N|               107|                  4|           2|        6.0|  0.0|    0.5|       0.0|         0.0|                  0.3|         6.8|                null|\n",
      "|        4|2019-01-09 15:08:02|2019-01-09 15:20:17|              1|          2.5|           1|                 N|               143|                158|           1|       11.0|  0.0|    0.5|       3.0|         0.0|                  0.3|        14.8|                null|\n",
      "|        5|2019-01-25 18:49:51|2019-01-25 18:56:44|              1|          0.8|           1|                 N|               246|                 90|           1|        6.5|  1.0|    0.5|      1.65|         0.0|                  0.3|        9.95|                 0.0|\n",
      "|        6|2019-01-26 18:20:17|2019-01-26 18:39:27|              1|          1.3|           1|                 N|               144|                 45|           2|       10.5|  0.0|    0.5|       0.0|         0.0|                  0.3|        11.3|                 0.0|\n",
      "|        8|2019-01-26 18:20:17|2019-01-26 18:39:27|              1|          1.3|           1|                 N|               144|                 45|           2|       10.5|  0.0|    0.5|       0.0|         0.0|                  0.3|        11.3|                 0.0|\n",
      "+---------+-------------------+-------------------+---------------+-------------+------------+------------------+------------------+-------------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from actual_table\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1085e177",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
